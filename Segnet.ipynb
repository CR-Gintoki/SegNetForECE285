{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Segnet network, default VGG16 encoder - VGG16 decoder\n",
    "\n",
    "Encoder given to front_layer\n",
    "Decoder given to back_layer\n",
    "\n",
    "upsampling default unmax-pooling\n",
    "may also try deconv, upsampling (preserved)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# size of the max pooling need to be optimize when can not divided by 2\n",
    "'''\n",
    "two methods:\n",
    "    1. fix the size of images, given to net as args, saved size in init.\n",
    "        Need image preprocessing (resize) to the target size.\n",
    "    \n",
    "    2. Unfixed size, calculate in forward prop and save in list, may take time.\n",
    "'''\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self, front_layer = [(64,),(64,),'M',(128,),(128,),'M',(256,),(256,),(256,),\\\n",
    "                                      'M', (512,),(512,),(512,),'M',(512,),(512,),(512,),'M'],\n",
    "                 back_layer = [('I', 512),'UM',(512,), (512,), (512,), 'UM', (512,), (512,), (256,), 'UM',\\\n",
    "                              (256,), (256,),(128,), 'UM', (128,),(64,),'UM',(64,), (64,) ],\n",
    "         class_num = 20,\n",
    "         use_BN = True,\n",
    "         upsampling = 'UM', # 'Deconv', 'USample'\n",
    "         img_size = (900,600)\n",
    "         ):\n",
    "        super(SegNet, self).__init__()\n",
    "        \n",
    "        self.class_num = class_num\n",
    "        self.use_BN = use_BN\n",
    "        \n",
    "        self.front_process = self.make_cnn_layers(front_layer, batch_norm= self.use_BN)\n",
    "        self.back_process = self.make_cnn_layers(back_layer, batch_norm = self.use_BN)\n",
    "        self.last_process = self.make_cnn_layers([('I',back_layer[-1][0]), (class_num,)])\n",
    "        self.img_size = img_size\n",
    "        self.upsampling = upsampling\n",
    "        \n",
    "        \n",
    "        m,n = img_size\n",
    "        self.size_lst = [(m,n)]\n",
    "        for i in front_layer:\n",
    "            if i == 'M':\n",
    "                m,n = m//2, n//2\n",
    "                self.size_lst.append((m,n))\n",
    "                if m <= 0 or n <= 0:\n",
    "                    raise Exception('Wrong Dimention or too many maxpooling!')\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        idx_lst = []\n",
    "        for layers in self.front_process:\n",
    "            if isinstance(layers, nn.MaxPool2d):\n",
    "                x,idx = layers(x)\n",
    "                idx_lst.append(idx)\n",
    "                print(x.size())\n",
    "            else:\n",
    "                x = layers(x)\n",
    "            \n",
    "            \n",
    "        print('\\nmiddle \\n')\n",
    "        \n",
    "        if self.upsampling == 'UM':\n",
    "            UM_cnt = -2\n",
    "        \n",
    "        for layers in self.back_process:\n",
    "            \n",
    "            if isinstance(layers, nn.MaxUnpool2d):\n",
    "                \n",
    "                x = layers(x, idx_lst.pop(-1),\n",
    "                           output_size=torch.Size([x.shape[0],\n",
    "                                                   x.shape[1],\n",
    "                                                   self.size_lst[UM_cnt][0],\n",
    "                                                   self.size_lst[UM_cnt][1]]))\n",
    "                print(x.size())\n",
    "                UM_cnt -= 1\n",
    "                \n",
    "            else:\n",
    "                x = layers(x)\n",
    "            \n",
    "        x = self.last_process(x)\n",
    "        print(x.size())\n",
    "        return x\n",
    "\n",
    "    def make_cnn_layers(self, cfg, batch_norm=True):\n",
    "\n",
    "        '''\n",
    "        The input should be a list, the elements should be tuples. \n",
    "        based on pytorch docs:\n",
    "        Parameters:\n",
    "            in_channels (int) – Number of channels in the input image\n",
    "            out_channels (int) – Number of channels produced by the convolution\n",
    "            kernel_size (int or tuple) – Size of the convolving kernel\n",
    "            stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "            padding (int or tuple, optional) – Zero-padding added to both sides of the input. Default: 1\n",
    "            dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1\n",
    "            groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1\n",
    "            bias (bool, optional) – If True, adds a learnable bias to the output. Default: True\n",
    "\n",
    "        For convolution layers, the order is:\n",
    "        (out_channels (int), kernel_size (int or tuple, optional, default = 3), \n",
    "        stride (int or tuple, optional), padding (int or tuple, optional), dilation (int or tuple, optional))\n",
    "        if input is less than 0 (ie. -1) then will use default value.\n",
    "\n",
    "        For maxpooling layer:\n",
    "        'M', if need more argument, modify as needed.\n",
    "\n",
    "        5/3/2018 C.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2,return_indices=True)]\n",
    "            \n",
    "            elif v == 'UM':\n",
    "                layers += [nn.MaxUnpool2d(kernel_size=2, stride=2)]\n",
    "                \n",
    "            elif v == 'Deconv':\n",
    "                raise Exception('Deconvolution network not written yet')\n",
    "                \n",
    "            elif v == 'Usample':\n",
    "                raise Exception('Upsampling network not written yet')\n",
    "            \n",
    "            elif v[0] == 'I':\n",
    "                in_channels = v[1]\n",
    "\n",
    "            else:\n",
    "                v_len = len(v)\n",
    "                ker_size = 3\n",
    "                stride_val = 1\n",
    "                padding_val = 1\n",
    "                dialtion_val = 1\n",
    "\n",
    "                out_channels = v[0]\n",
    "                if v_len >= 2:\n",
    "                    if v[1] > 0:\n",
    "                        ker_size = v[1]\n",
    "                    if v_len >= 3:\n",
    "                        if v[2] > 0:\n",
    "                            stride_val = v[2]\n",
    "                        if v_len >= 4:\n",
    "                            if v[3] > 0:\n",
    "                                padding_val = v[3]\n",
    "                            if v_len >= 5:\n",
    "                                if v[4] > 0:\n",
    "                                    dialtion_val = v[4]\n",
    "\n",
    "\n",
    "                conv2d = nn.Conv2d(in_channels, out_channels, kernel_size = ker_size, stride = stride_val,\n",
    "                                   padding = padding_val, dilation = dialtion_val)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v[0]), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v[0]\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 900, 450])\n",
      "torch.Size([1, 64, 450, 225])\n",
      "torch.Size([1, 128, 225, 112])\n",
      "torch.Size([1, 256, 112, 56])\n",
      "torch.Size([1, 512, 56, 28])\n",
      "torch.Size([1, 512, 28, 14])\n",
      "\n",
      "middle \n",
      "\n",
      "torch.Size([1, 512, 56, 28])\n",
      "torch.Size([1, 512, 112, 56])\n",
      "torch.Size([1, 256, 225, 112])\n",
      "torch.Size([1, 128, 450, 225])\n",
      "torch.Size([1, 64, 900, 450])\n",
      "torch.Size([1, 20, 900, 450])\n"
     ]
    }
   ],
   "source": [
    "img_size = (900,450)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "test_net  = SegNet(img_size=img_size).cuda()\n",
    "input_test = torch.zeros((1,3,900,450)).cuda()\n",
    "print(input_test.size())\n",
    "out_test = test_net(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
